training:
    per_device_train_batch_size: 2
    gradient_accumulation_steps: 4
    warmup_steps: 5
    max_steps: 60
    learning_rate: 2e-4
    fp16: True
    logging_steps: 1
    optim: paged_adamw_8bit
    weight_decay: 0.01
    lr_scheduler_type: linear
    seed: 3407
    output_dir: "./llama-bengali-empathy"
    report_to: None
